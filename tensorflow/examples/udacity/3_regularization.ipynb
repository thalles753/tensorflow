{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (200000, 28, 28), (200000,))\n",
      "('Validation set', (10000, 28, 28), (10000,))\n",
      "('Test set', (18724, 28, 28), (18724,))\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (200000, 784), (200000, 10))\n",
      "('Validation set', (10000, 784), (10000, 10))\n",
      "('Test set', (18724, 784), (18724, 10))\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compue the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce L2 Regularization for the Multinomial Logistic Regression model in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# define the input variable\n",
    "# the input variable will receive the image's pixels for every batch\n",
    "tf_train_dataset = tf.placeholder(tf.float32, [None, image_size * image_size]) # batch_sizex784 matrix \n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "\n",
    "tf_valid_dataset = tf.constant(valid_dataset)\n",
    "tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "# define the weights' parameters\n",
    "W = tf.Variable(tf.truncated_normal([image_size*image_size, num_labels])) # 784x10 Matrix \n",
    "\n",
    "# define the biases\n",
    "b = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "logits = tf.matmul(tf_train_dataset, W) + b\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "regularizers = tf.nn.l2_loss(W)\n",
    "loss += 5e-4 * regularizers \n",
    "\n",
    "# optimize the loss function using gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "train_predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# run the validation dataset using the trained netword (weights and biases)\n",
    "valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, W) + b)\n",
    "\n",
    "# run the test dataset in the trained netword (weights and biases)\n",
    "test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, W) + b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the model\n",
    "\n",
    "- It is worth noting that the validation dataset is ran in each interation of the traing phase. So as the batches go and improve the parameters, the valid dataset is ran over the graph using these weights and biases recently improved.\n",
    "\n",
    "- At the end, the test dataset in ran over the network to evaluate the network as a whole since the weights and biases have already finished training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 19.405302\n",
      "Minibatch accuracy 5.5%\n",
      "Validation accuracy 10.6%\n",
      "Minibatch loss at step 500: 2.438282\n",
      "Minibatch accuracy 73.4%\n",
      "Validation accuracy 75.3%\n",
      "Minibatch loss at step 1000: 1.817489\n",
      "Minibatch accuracy 75.0%\n",
      "Validation accuracy 77.1%\n",
      "Minibatch loss at step 1500: 1.412781\n",
      "Minibatch accuracy 79.7%\n",
      "Validation accuracy 79.1%\n",
      "Minibatch loss at step 2000: 1.452242\n",
      "Minibatch accuracy 78.1%\n",
      "Validation accuracy 79.2%\n",
      "Minibatch loss at step 2500: 1.007788\n",
      "Minibatch accuracy 82.8%\n",
      "Validation accuracy 80.3%\n",
      "Minibatch loss at step 3000: 1.046847\n",
      "Minibatch accuracy 77.3%\n",
      "Validation accuracy 80.1%\n",
      "Test accuracy: 87.9%\n"
     ]
    }
   ],
   "source": [
    "# properly initialize the tensorflow variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# initialize the model in run the operation to initialize the variables\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    \n",
    "    opt, l, predictions = sess.run(\n",
    "      [optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy %.1f%%\" % accuracy(valid_prediction.eval(session=sess), valid_labels))\n",
    "        \n",
    "print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(session=sess), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets introduce the L2 regularization to a 2-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# define the input variable\n",
    "# the input variable will receive the image's pixels for every batch\n",
    "tf_train_dataset = tf.placeholder(tf.float32, [None, image_size * image_size]) # [128 x 784] matrix \n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "\n",
    "# load the valid and test datasets\n",
    "tf_valid_dataset = tf.constant(valid_dataset)\n",
    "tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "# define the hidden layer size\n",
    "hidden_layer_size = 1024\n",
    "\n",
    "# define the weights (parameters) of the first layer\n",
    "W_layer1 = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_layer_size])) # [784x1024] Matrix \n",
    "\n",
    "# define the biases for the firtst layer\n",
    "b_layer1 = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "\n",
    "# training computation\n",
    "hidden_layer = tf.matmul(tf_train_dataset, W_layer1) + b_layer1 # [128 x 1024] Matrix\n",
    "\n",
    "# apply the relu (Rectified Linear Regression) function \n",
    "relu_hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "# define the parameters for the second layer\n",
    "W_layer2 = tf.Variable(tf.truncated_normal([hidden_layer_size, num_labels])) # [1024 x 10] Matrix\n",
    "\n",
    "# define the biases for the second layer \n",
    "b_layer2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "# training computation\n",
    "logits = tf.matmul(relu_hidden_layer, W_layer2) + b_layer2 # [128 x 10]\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "regularizers = tf.nn.l2_loss(W_layer1) + tf.nn.l2_loss(W_layer2)\n",
    "loss += 5e-4 * regularizers \n",
    "\n",
    "# optimize the loss function using gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "train_predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# run the validation dataset using the trained netword (weights and biases)\n",
    "valid_prediction_hidden_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, W_layer1) + b_layer1)\n",
    "valid_prediction = tf.nn.softmax(tf.matmul(valid_prediction_hidden_layer, W_layer2) + b_layer2)\n",
    "\n",
    "# run the test dataset in the trained netword (weights and biases)\n",
    "test_prediction_hidden_layer = tf.nn.relu(tf.matmul(tf_test_dataset, W_layer1) + b_layer1)\n",
    "test_prediction = tf.nn.softmax(tf.matmul(test_prediction_hidden_layer, W_layer2) + b_layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 602.456726\n",
      "Minibatch accuracy 9.4%\n",
      "Validation accuracy 15.2%\n",
      "Subset: 0 - 128\n",
      "Minibatch loss at step 500: 175.481293\n",
      "Minibatch accuracy 77.3%\n",
      "Validation accuracy 78.7%\n",
      "Subset: 64000 - 64128\n",
      "Test accuracy: 85.2%\n"
     ]
    }
   ],
   "source": [
    "# properly initialize the tensorflow variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# initialize the model in run the operation to initialize the variables\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    \n",
    "    opt, l, predictions = sess.run(\n",
    "      [optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy %.1f%%\" % accuracy(valid_prediction.eval(session=sess), valid_labels))\n",
    "        print(\"Subset: %d - %d\" % (offset, offset + batch_size))\n",
    "        \n",
    "print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(session=sess), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# define the input variable\n",
    "# the input variable will receive the image's pixels for every batch\n",
    "tf_train_dataset = tf.placeholder(tf.float32, [None, image_size * image_size]) # [128 x 784] matrix \n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "\n",
    "# load the valid and test datasets\n",
    "tf_valid_dataset = tf.constant(valid_dataset)\n",
    "tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "# define the hidden layer size\n",
    "hidden_layer_size = 1024\n",
    "\n",
    "# define the weights (parameters) of the first layer\n",
    "W_layer1 = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_layer_size])) # [784x1024] Matrix \n",
    "\n",
    "# define the biases for the firtst layer\n",
    "b_layer1 = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "\n",
    "# training computation\n",
    "hidden_layer = tf.matmul(tf_train_dataset, W_layer1) + b_layer1 # [128 x 1024] Matrix\n",
    "\n",
    "# apply the relu (Rectified Linear Regression) function \n",
    "relu_hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "# define the parameters for the second layer\n",
    "W_layer2 = tf.Variable(tf.truncated_normal([hidden_layer_size, num_labels])) # [1024 x 10] Matrix\n",
    "\n",
    "# define the biases for the second layer \n",
    "b_layer2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "# training computation\n",
    "logits = tf.matmul(relu_hidden_layer, W_layer2) + b_layer2 # [128 x 10]\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "# regularizers = tf.nn.l2_loss(W_layer1) + tf.nn.l2_loss(W_layer2)\n",
    "# loss += 5e-4 * regularizers \n",
    "\n",
    "# optimize the loss function using gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "train_predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# run the validation dataset using the trained netword (weights and biases)\n",
    "valid_prediction_hidden_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, W_layer1) + b_layer1)\n",
    "valid_prediction = tf.nn.softmax(tf.matmul(valid_prediction_hidden_layer, W_layer2) + b_layer2)\n",
    "\n",
    "# run the test dataset in the trained netword (weights and biases)\n",
    "test_prediction_hidden_layer = tf.nn.relu(tf.matmul(tf_test_dataset, W_layer1) + b_layer1)\n",
    "test_prediction = tf.nn.softmax(tf.matmul(test_prediction_hidden_layer, W_layer2) + b_layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 427.688782\n",
      "Minibatch accuracy 10.2%\n",
      "Validation accuracy 12.5%\n",
      "Minibatch loss at step 20: 70.872391\n",
      "Minibatch accuracy 65.6%\n",
      "Validation accuracy 68.8%\n",
      "Minibatch loss at step 40: 53.254601\n",
      "Minibatch accuracy 69.5%\n",
      "Validation accuracy 71.8%\n",
      "Minibatch loss at step 60: 56.218655\n",
      "Minibatch accuracy 71.1%\n",
      "Validation accuracy 73.4%\n",
      "Minibatch loss at step 80: 40.858280\n",
      "Minibatch accuracy 76.6%\n",
      "Validation accuracy 75.2%\n",
      "Minibatch loss at step 100: 55.675922\n",
      "Minibatch accuracy 71.9%\n",
      "Validation accuracy 71.8%\n",
      "Test accuracy: 78.0%\n"
     ]
    }
   ],
   "source": [
    "# properly initialize the tensorflow variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# initialize the model in run the operation to initialize the variables\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "num_steps = 101\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    \n",
    "    opt, l, predictions = sess.run(\n",
    "      [optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 20 == 0):\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy %.1f%%\" % accuracy(valid_prediction.eval(session=sess), valid_labels))\n",
    "        \n",
    "print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(session=sess), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing only 101 (from 0 to 100) batches, each one of size 128, the final test accuracy was 82.3% (Without Regularization). If this approach was to overfit the model, I was expecting that the Minibaches accuracy went up. However, at step 100, the Minibatch accuracy was 74.2%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
