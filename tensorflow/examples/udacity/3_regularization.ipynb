{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (200000, 28, 28), (200000,))\n",
      "('Validation set', (10000, 28, 28), (10000,))\n",
      "('Test set', (18724, 28, 28), (18724,))\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (200000, 784), (200000, 10))\n",
      "('Validation set', (10000, 784), (10000, 10))\n",
      "('Test set', (18724, 784), (18724, 10))\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compue the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce L2 Regularization for the Multinomial Logistic Regression model in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# define the input variable\n",
    "# the input variable will receive the image's pixels for every batch\n",
    "tf_train_dataset = tf.placeholder(tf.float32, [None, image_size * image_size]) # batch_sizex784 matrix \n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "\n",
    "tf_valid_dataset = tf.constant(valid_dataset)\n",
    "tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "# define the weights' parameters\n",
    "W = tf.Variable(tf.truncated_normal([image_size*image_size, num_labels])) # 784x10 Matrix \n",
    "\n",
    "# define the biases\n",
    "b = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "logits = tf.matmul(tf_train_dataset, W) + b\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "regularizers = tf.nn.l2_loss(W)\n",
    "loss += 5e-4 * regularizers \n",
    "\n",
    "# optimize the loss function using gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "train_predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# run the validation dataset using the trained netword (weights and biases)\n",
    "valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, W) + b)\n",
    "\n",
    "# run the test dataset in the trained netword (weights and biases)\n",
    "test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, W) + b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the model\n",
    "\n",
    "- It is worth noting that the validation dataset is ran in each interation of the traing phase. So as the batches go and improve the parameters, the valid dataset is ran over the graph using these weights and biases recently improved.\n",
    "\n",
    "- At the end, the test dataset in ran over the network to evaluate the network as a whole since the weights and biases have already finished training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 22.868132\n",
      "Minibatch accuracy 4.7%\n",
      "Validation accuracy 6.7%\n",
      "Minibatch loss at step 500: 2.752666\n",
      "Minibatch accuracy 71.1%\n",
      "Validation accuracy 75.6%\n",
      "Minibatch loss at step 1000: 1.913516\n",
      "Minibatch accuracy 73.4%\n",
      "Validation accuracy 77.3%\n",
      "Minibatch loss at step 1500: 1.365192\n",
      "Minibatch accuracy 82.0%\n",
      "Validation accuracy 78.1%\n",
      "Minibatch loss at step 2000: 1.247186\n",
      "Minibatch accuracy 71.1%\n",
      "Validation accuracy 79.0%\n",
      "Minibatch loss at step 2500: 1.049087\n",
      "Minibatch accuracy 77.3%\n",
      "Validation accuracy 79.8%\n",
      "Minibatch loss at step 3000: 0.922375\n",
      "Minibatch accuracy 79.7%\n",
      "Validation accuracy 80.1%\n",
      "Test accuracy: 87.5%\n"
     ]
    }
   ],
   "source": [
    "# properly initialize the tensorflow variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# initialize the model in run the operation to initialize the variables\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    \n",
    "    opt, l, predictions = sess.run(\n",
    "      [optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy %.1f%%\" % accuracy(valid_prediction.eval(session=sess), valid_labels))\n",
    "        \n",
    "print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(session=sess), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets introduce the L2 regularization to a 2-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# define the input variable\n",
    "# the input variable will receive the image's pixels for every batch\n",
    "tf_train_dataset = tf.placeholder(tf.float32, [None, image_size * image_size]) # [128 x 784] matrix \n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "\n",
    "# load the valid and test datasets\n",
    "tf_valid_dataset = tf.constant(valid_dataset)\n",
    "tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "# define the hidden layer size\n",
    "hidden_layer_size = 1024\n",
    "\n",
    "# define the weights (parameters) of the first layer\n",
    "W_layer1 = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_layer_size])) # [784x1024] Matrix \n",
    "\n",
    "# define the biases for the firtst layer\n",
    "b_layer1 = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "\n",
    "# training computation\n",
    "hidden_layer = tf.matmul(tf_train_dataset, W_layer1) + b_layer1 # [128 x 1024] Matrix\n",
    "\n",
    "# apply the relu (Rectified Linear Regression) function \n",
    "relu_hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "# define the parameters for the second layer\n",
    "W_layer2 = tf.Variable(tf.truncated_normal([hidden_layer_size, num_labels])) # [1024 x 10] Matrix\n",
    "\n",
    "# define the biases for the second layer \n",
    "b_layer2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "# training computation\n",
    "logits = tf.matmul(relu_hidden_layer, W_layer2) + b_layer2 # [128 x 10]\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "regularizers = tf.nn.l2_loss(W_layer1) + tf.nn.l2_loss(W_layer2)\n",
    "loss += 5e-4 * regularizers \n",
    "\n",
    "# optimize the loss function using gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "train_predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# run the validation dataset using the trained netword (weights and biases)\n",
    "valid_prediction_hidden_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, W_layer1) + b_layer1)\n",
    "valid_prediction = tf.nn.softmax(tf.matmul(valid_prediction_hidden_layer, W_layer2) + b_layer2)\n",
    "\n",
    "# run the test dataset in the trained netword (weights and biases)\n",
    "test_prediction_hidden_layer = tf.nn.relu(tf.matmul(tf_test_dataset, W_layer1) + b_layer1)\n",
    "test_prediction = tf.nn.softmax(tf.matmul(test_prediction_hidden_layer, W_layer2) + b_layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 487.253387\n",
      "Minibatch accuracy 7.8%\n",
      "Validation accuracy 24.9%\n",
      "Subset: 0 - 128\n",
      "Minibatch loss at step 500: 179.847626\n",
      "Minibatch accuracy 81.2%\n",
      "Validation accuracy 79.5%\n",
      "Subset: 64000 - 64128\n",
      "Minibatch loss at step 1000: 164.702271\n",
      "Minibatch accuracy 82.8%\n",
      "Validation accuracy 81.7%\n",
      "Subset: 128000 - 128128\n",
      "Minibatch loss at step 1500: 161.709427\n",
      "Minibatch accuracy 80.5%\n",
      "Validation accuracy 80.7%\n",
      "Subset: 192000 - 192128\n",
      "Minibatch loss at step 2000: 151.466873\n",
      "Minibatch accuracy 79.7%\n",
      "Validation accuracy 81.9%\n",
      "Subset: 56128 - 56256\n",
      "Minibatch loss at step 2500: 143.636658\n",
      "Minibatch accuracy 83.6%\n",
      "Validation accuracy 83.3%\n",
      "Subset: 120128 - 120256\n",
      "Minibatch loss at step 3000: 142.981293\n",
      "Minibatch accuracy 81.2%\n",
      "Validation accuracy 83.0%\n",
      "Subset: 184128 - 184256\n",
      "Test accuracy: 89.5%\n"
     ]
    }
   ],
   "source": [
    "# properly initialize the tensorflow variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# initialize the model in run the operation to initialize the variables\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    \n",
    "    opt, l, predictions = sess.run(\n",
    "      [optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy %.1f%%\" % accuracy(valid_prediction.eval(session=sess), valid_labels))\n",
    "        \n",
    "print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(session=sess), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4*128\n",
    "\n",
    "# define the input variable\n",
    "# the input variable will receive the image's pixels for every batch\n",
    "tf_train_dataset = tf.placeholder(tf.float32, [None, image_size * image_size]) # [128 x 784] matrix \n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "\n",
    "# load the valid and test datasets\n",
    "tf_valid_dataset = tf.constant(valid_dataset)\n",
    "tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "# define the hidden layer size\n",
    "hidden_layer_size = 1024\n",
    "\n",
    "# define the weights (parameters) of the first layer\n",
    "W_layer1 = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_layer_size])) # [784x1024] Matrix \n",
    "\n",
    "# define the biases for the firtst layer\n",
    "b_layer1 = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "\n",
    "# training computation\n",
    "hidden_layer = tf.matmul(tf_train_dataset, W_layer1) + b_layer1 # [128 x 1024] Matrix\n",
    "\n",
    "# apply the relu (Rectified Linear Regression) function \n",
    "relu_hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "# define the parameters for the second layer\n",
    "W_layer2 = tf.Variable(tf.truncated_normal([hidden_layer_size, num_labels])) # [1024 x 10] Matrix\n",
    "\n",
    "# define the biases for the second layer \n",
    "b_layer2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "# training computation\n",
    "logits = tf.matmul(relu_hidden_layer, W_layer2) + b_layer2 # [128 x 10]\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "# regularizers = tf.nn.l2_loss(W_layer1) + tf.nn.l2_loss(W_layer2)\n",
    "# loss += 5e-4 * regularizers \n",
    "\n",
    "# optimize the loss function using gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "train_predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# run the validation dataset using the trained netword (weights and biases)\n",
    "valid_prediction_hidden_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, W_layer1) + b_layer1)\n",
    "valid_prediction = tf.nn.softmax(tf.matmul(valid_prediction_hidden_layer, W_layer2) + b_layer2)\n",
    "\n",
    "# run the test dataset in the trained netword (weights and biases)\n",
    "test_prediction_hidden_layer = tf.nn.relu(tf.matmul(tf_test_dataset, W_layer1) + b_layer1)\n",
    "test_prediction = tf.nn.softmax(tf.matmul(test_prediction_hidden_layer, W_layer2) + b_layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 273.788513\n",
      "Minibatch accuracy 11.5%\n",
      "Validation accuracy 24.4%\n",
      "Minibatch loss at step 500: 0.000007\n",
      "Minibatch accuracy 100.0%\n",
      "Validation accuracy 60.3%\n",
      "Minibatch loss at step 1000: 0.000004\n",
      "Minibatch accuracy 100.0%\n",
      "Validation accuracy 60.3%\n",
      "Minibatch loss at step 1500: 0.000003\n",
      "Minibatch accuracy 100.0%\n",
      "Validation accuracy 60.2%\n",
      "Minibatch loss at step 2000: 0.000003\n",
      "Minibatch accuracy 100.0%\n",
      "Validation accuracy 60.2%\n",
      "Minibatch loss at step 2500: 0.000002\n",
      "Minibatch accuracy 100.0%\n",
      "Validation accuracy 60.3%\n",
      "Minibatch loss at step 3000: 0.000002\n",
      "Minibatch accuracy 100.0%\n",
      "Validation accuracy 60.3%\n",
      "Test accuracy: 67.3%\n"
     ]
    }
   ],
   "source": [
    "# properly initialize the tensorflow variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# initialize the model in run the operation to initialize the variables\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    # Lets try to overfit the model using a small portion of the global training set\n",
    "    # offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[0:(batch_size), :]\n",
    "    batch_labels = train_labels[0:(batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    \n",
    "    opt, l, predictions = sess.run(\n",
    "      [optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy %.1f%%\" % accuracy(valid_prediction.eval(session=sess), valid_labels))\n",
    "        \n",
    "print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(session=sess), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing only 101 (from 0 to 100) batches, each one of size 128, the final test accuracy was 82.3% (Without Regularization). If this approach was to overfit the model, I was expecting that the Minibaches accuracy went up. However, at step 100, the Minibatch accuracy was 74.2%.\n",
    "\n",
    "A second approach is to shrink the traning data to be the size of a few batches and run a Gradient descent approach over that data many times. Restricting the training dataset to 128x4 = 512 training data points, and running it over 3001 step (each of them applying the same 512 dataset), we can see the Minibatch accuracy be 100.0%. Which makes totally sense, now we have a overfitted model in that dataset. The test accuracy was 67.3%, which also makes sense since an overfitted model cannot generalize well.\n",
    "\n",
    "- Using drop out in the same example, with 0.5 probability for on training only. The Test Accuracy jumps to 85.4%. That proves the assertion that dropout alleviates the model overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# define the input variable\n",
    "# the input variable will receive the image's pixels for every batch\n",
    "tf_train_dataset = tf.placeholder(tf.float32, [None, image_size * image_size]) # [128 x 784] matrix \n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "\n",
    "# load the valid and test datasets\n",
    "tf_valid_dataset = tf.constant(valid_dataset)\n",
    "tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "# define the hidden layer size\n",
    "hidden_layer_size = 1024\n",
    "\n",
    "# define the weights (parameters) of the first layer\n",
    "W_layer1 = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_layer_size])) # [784x1024] Matrix \n",
    "\n",
    "# define the biases for the firtst layer\n",
    "b_layer1 = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "\n",
    "# training computation\n",
    "hidden_layer = tf.matmul(tf_train_dataset, W_layer1) + b_layer1 # [128 x 1024] Matrix\n",
    "\n",
    "# apply the relu (Rectified Linear Regression) function \n",
    "relu_hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "# to alleviate overfitting, lets add drop out in between the layers\n",
    "# i.e. when the activations from the first layer are flowing to the\n",
    "# second.\n",
    "# lets create a variable to record the probability that an activation\n",
    "# will be dropped out or not.\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "relu_hidden_layer = tf.nn.dropout(relu_hidden_layer, keep_prob)\n",
    "\n",
    "# define the parameters for the second layer\n",
    "W_layer2 = tf.Variable(tf.truncated_normal([hidden_layer_size, num_labels])) # [1024 x 10] Matrix\n",
    "\n",
    "# define the biases for the second layer \n",
    "b_layer2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "# training computation\n",
    "logits = tf.matmul(relu_hidden_layer, W_layer2) + b_layer2 # [128 x 10]\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "regularizers = tf.nn.l2_loss(W_layer1) + tf.nn.l2_loss(W_layer2)\n",
    "loss += 5e-4 * regularizers \n",
    "\n",
    "# optimize the loss function using gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "train_predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# run the validation dataset using the trained netword (weights and biases)\n",
    "valid_prediction_hidden_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, W_layer1) + b_layer1)\n",
    "valid_prediction = tf.nn.softmax(tf.matmul(valid_prediction_hidden_layer, W_layer2) + b_layer2)\n",
    "\n",
    "# run the test dataset in the trained netword (weights and biases)\n",
    "test_prediction_hidden_layer = tf.nn.relu(tf.matmul(tf_test_dataset, W_layer1) + b_layer1)\n",
    "test_prediction = tf.nn.softmax(tf.matmul(test_prediction_hidden_layer, W_layer2) + b_layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 596.897095\n",
      "Minibatch accuracy 35.2%\n",
      "Validation accuracy 23.6%\n",
      "Minibatch loss at step 500: 145.476227\n",
      "Minibatch accuracy 86.7%\n",
      "Validation accuracy 81.3%\n",
      "Minibatch loss at step 1000: 98.985794\n",
      "Minibatch accuracy 85.9%\n",
      "Validation accuracy 79.7%\n",
      "Minibatch loss at step 1500: 78.144447\n",
      "Minibatch accuracy 85.9%\n",
      "Validation accuracy 80.7%\n",
      "Minibatch loss at step 2000: 58.958641\n",
      "Minibatch accuracy 84.4%\n",
      "Validation accuracy 81.1%\n",
      "Minibatch loss at step 2500: 45.544498\n",
      "Minibatch accuracy 90.6%\n",
      "Validation accuracy 81.8%\n",
      "Minibatch loss at step 3000: 34.152397\n",
      "Minibatch accuracy 86.7%\n",
      "Validation accuracy 83.2%\n",
      "Test accuracy: 90.0%\n"
     ]
    }
   ],
   "source": [
    "# properly initialize the tensorflow variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# initialize the model in run the operation to initialize the variables\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob: 0.5}\n",
    "    \n",
    "    opt, l, predictions = sess.run(\n",
    "      [optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy %.1f%%\" % accuracy(\n",
    "                train_predictions.eval(\n",
    "                    session=sess, feed_dict = {tf_train_dataset : batch_data, \n",
    "                                               tf_train_labels : batch_labels, \n",
    "                                               keep_prob: 1.0}), batch_labels))\n",
    "        print(\"Validation accuracy %.1f%%\" % \n",
    "              accuracy(valid_prediction.eval(session=sess), valid_labels))\n",
    "        \n",
    "print(\"Test accuracy: %.1f%%\" % \n",
    "      accuracy(test_prediction.eval(session=sess), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to increase the accuracy by adding more layers and by using learning rate decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# define the input variable\n",
    "# the input variable will receive the image's pixels for every batch\n",
    "tf_train_dataset = tf.placeholder(tf.float32, [None, image_size * image_size]) # [128 x 784] matrix \n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "\n",
    "# placeholder to determine the percentage of data that will be dropped out\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# load the valid and test datasets\n",
    "tf_valid_dataset = tf.constant(valid_dataset)\n",
    "tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "##############################################################\n",
    "## Layer 1\n",
    "##############################################################\n",
    "# define the hidden layer size\n",
    "hidden_layer_size = 1024\n",
    "\n",
    "# define the weights (parameters) of the first layer\n",
    "W_layer1 = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_layer_size], stddev=0.03)) # [784x1024] Matrix \n",
    "\n",
    "# define the biases for the firtst layer\n",
    "b_layer1 = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "\n",
    "# training computation\n",
    "hidden_layer = tf.matmul(tf_train_dataset, W_layer1) + b_layer1 # [128 x 1024] Matrix\n",
    "\n",
    "# apply the relu (Rectified Linear Regression) function \n",
    "relu_hidden_layer = tf.nn.dropout(tf.nn.relu(hidden_layer), keep_prob)\n",
    "\n",
    "##############################################################\n",
    "## Layer 2\n",
    "##############################################################\n",
    "# define the hidden layer size\n",
    "hidden_layer2_size = 1024\n",
    "\n",
    "# define the weights (parameters) of the first layer\n",
    "W_layer2 = tf.Variable(tf.truncated_normal([hidden_layer_size, hidden_layer2_size], stddev=0.03)) # [1024x2048] Matrix \n",
    "\n",
    "# define the biases for the firtst layer\n",
    "b_layer2 = tf.Variable(tf.zeros([hidden_layer2_size])) # [2048]\n",
    "\n",
    "# training computation\n",
    "hidden_layer2 = tf.matmul(relu_hidden_layer, W_layer2) + b_layer2 # [128 x 2048] Matrix\n",
    "\n",
    "# apply the relu (Rectified Linear Regression) function \n",
    "relu_hidden_layer2 = tf.nn.dropout(tf.nn.relu(hidden_layer2), keep_prob)\n",
    "\n",
    "##############################################################\n",
    "## Layer 3\n",
    "##############################################################\n",
    "# define the hidden layer size\n",
    "# hidden_layer3_size = 1024\n",
    "\n",
    "# define the weights (parameters) of the first layer\n",
    "# W_layer3 = tf.Variable(tf.truncated_normal([hidden_layer2_size, hidden_layer3_size], stddev=0.03)) # [2048x2048] Matrix \n",
    "\n",
    "# define the biases for the firtst layer\n",
    "# b_layer3 = tf.Variable(tf.zeros([hidden_layer3_size])) # [2048]\n",
    "\n",
    "# training computation\n",
    "# hidden_layer3 = tf.matmul(relu_hidden_layer2, W_layer3) + b_layer3 # [128 x 2048] Matrix\n",
    "\n",
    "# apply the relu (Rectified Linear Regression) function \n",
    "# relu_hidden_layer3 = tf.nn.dropout(tf.nn.relu(hidden_layer3), keep_prob)\n",
    "\n",
    "##############################################################\n",
    "## Layer 4\n",
    "##############################################################\n",
    "\n",
    "# define the parameters for the second layer\n",
    "W_layer3 = tf.Variable(tf.truncated_normal([hidden_layer2_size, num_labels], stddev=0.03)) # [1024 x 10] Matrix\n",
    "\n",
    "# define the biases for the second layer \n",
    "b_layer3 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "# training computation\n",
    "logits = tf.matmul(relu_hidden_layer2, W_layer3) + b_layer3 # [128 x 10]\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "# apply regularization\n",
    "regularizers = tf.nn.l2_loss(W_layer1) + tf.nn.l2_loss(W_layer2) + tf.nn.l2_loss(W_layer3)\n",
    "loss += 5e-4 * regularizers \n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.3\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           decay_steps = batch_size, \n",
    "                                           decay_rate = 0.95, \n",
    "                                           staircase=True)\n",
    "\n",
    "# optimize the loss function using gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "train_predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# run the validation dataset using the trained netword (weights and biases)\n",
    "valid_prediction_hidden_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, W_layer1) + b_layer1)\n",
    "valid_prediction_hidden_layer = tf.nn.relu(tf.matmul(valid_prediction_hidden_layer, W_layer2) + b_layer2)\n",
    "#valid_prediction_hidden_layer = tf.nn.relu(tf.matmul(valid_prediction_hidden_layer, W_layer3) + b_layer3)\n",
    "valid_prediction = tf.nn.softmax(tf.matmul(valid_prediction_hidden_layer, W_layer3) + b_layer3)\n",
    "\n",
    "# run the test dataset in the trained netword (weights and biases)\n",
    "test_prediction_hidden_layer = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_test_dataset, W_layer1) + b_layer1), 1.0)\n",
    "test_prediction_hidden_layer = tf.nn.dropout(tf.nn.relu(tf.matmul(test_prediction_hidden_layer, W_layer2) + b_layer2), 1.0)\n",
    "#test_prediction_hidden_layer = tf.nn.dropout(tf.nn.relu(tf.matmul(test_prediction_hidden_layer, W_layer3) + b_layer3), 1.0)\n",
    "test_prediction = tf.nn.softmax(tf.matmul(test_prediction_hidden_layer, W_layer3) + b_layer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# properly initialize the tensorflow variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# initialize the model in run the operation to initialize the variables\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "num_steps = 3001\n",
    "learning_rate_decay = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob: 0.5}\n",
    "    \n",
    "    opt, l, l_rate, predictions = sess.run(\n",
    "      [optimizer, loss, learning_rate, train_predictions], feed_dict=feed_dict)\n",
    "    \n",
    "    \n",
    "    learning_rate_decay.append(l_rate)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy %.1f%%\" % accuracy(valid_prediction.eval(session=sess), valid_labels))\n",
    "        \n",
    "print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(session=sess), test_labels))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(learning_rate_decay)\n",
    "plt.grid(1)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
