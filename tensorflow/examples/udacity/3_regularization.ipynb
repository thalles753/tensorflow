{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (200000, 28, 28), (200000,))\n",
      "('Validation set', (10000, 28, 28), (10000,))\n",
      "('Test set', (18724, 28, 28), (18724,))\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (200000, 784), (200000, 10))\n",
      "('Validation set', (10000, 784), (10000, 10))\n",
      "('Test set', (18724, 784), (18724, 10))\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compue the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce L2 Regularization for the Multinomial Logistic Regression model in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# define the input variable\n",
    "# the input variable will receive the image's pixels for every batch\n",
    "tf_train_dataset = tf.placeholder(tf.float32, [None, image_size * image_size]) # batch_sizex784 matrix \n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "\n",
    "tf_valid_dataset = tf.constant(valid_dataset)\n",
    "tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "# define the weights' parameters\n",
    "W = tf.Variable(tf.truncated_normal([image_size*image_size, num_labels])) # 784x10 Matrix \n",
    "\n",
    "# define the biases\n",
    "b = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "logits = tf.matmul(tf_train_dataset, W) + b\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "regularizers = tf.nn.l2_loss(W)\n",
    "loss += 5e-4 * regularizers \n",
    "\n",
    "# optimize the loss function using gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "train_predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# run the validation dataset using the trained netword (weights and biases)\n",
    "valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, W) + b)\n",
    "\n",
    "# run the test dataset in the trained netword (weights and biases)\n",
    "test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, W) + b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the model\n",
    "\n",
    "- It is worth noting that the validation dataset is ran in each interation of the traing phase. So as the batches go and improve the parameters, the valid dataset is ran over the graph using these weights and biases recently improved.\n",
    "\n",
    "- At the end, the test dataset in ran over the network to evaluate the network as a whole since the weights and biases have already finished training\n",
    "\n",
    "- On the Assignment 2, the same multinomial logistic regressiton using SGD (Stochastic Gradient Descent) accomplished an accuracy of 85.6%. So, using L2 regularization we get an improvement of roughly 2.5% since the model now gets around 88.1% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 17.299664\n",
      "Minibatch accuracy 6.2%\n",
      "Validation accuracy 10.1%\n",
      "Minibatch loss at step 500: 2.236001\n",
      "Minibatch accuracy 76.6%\n",
      "Validation accuracy 75.1%\n",
      "Minibatch loss at step 1000: 1.811796\n",
      "Minibatch accuracy 75.8%\n",
      "Validation accuracy 76.8%\n",
      "Minibatch loss at step 1500: 1.225581\n",
      "Minibatch accuracy 77.3%\n",
      "Validation accuracy 78.8%\n",
      "Minibatch loss at step 2000: 1.408456\n",
      "Minibatch accuracy 78.9%\n",
      "Validation accuracy 79.1%\n",
      "Minibatch loss at step 2500: 0.966792\n",
      "Minibatch accuracy 81.2%\n",
      "Validation accuracy 80.4%\n",
      "Minibatch loss at step 3000: 1.071054\n",
      "Minibatch accuracy 75.8%\n",
      "Validation accuracy 80.4%\n",
      "Test accuracy: 88.1%\n"
     ]
    }
   ],
   "source": [
    "# properly initialize the tensorflow variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# initialize the model in run the operation to initialize the variables\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    \n",
    "    opt, l, predictions = sess.run(\n",
    "      [optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy %.1f%%\" % accuracy(valid_prediction.eval(session=sess), valid_labels))\n",
    "        \n",
    "print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(session=sess), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets introduce the L2 regularization to a 2-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# define the input variable\n",
    "# the input variable will receive the image's pixels for every batch\n",
    "tf_train_dataset = tf.placeholder(tf.float32, [None, image_size * image_size]) # [128 x 784] matrix \n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "\n",
    "# load the valid and test datasets\n",
    "tf_valid_dataset = tf.constant(valid_dataset)\n",
    "tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "# define the hidden layer size\n",
    "hidden_layer_size = 1024\n",
    "\n",
    "# define the weights (parameters) of the first layer\n",
    "W_layer1 = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_layer_size])) # [784x1024] Matrix \n",
    "\n",
    "# define the biases for the firtst layer\n",
    "b_layer1 = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "\n",
    "# training computation\n",
    "hidden_layer = tf.matmul(tf_train_dataset, W_layer1) + b_layer1 # [128 x 1024] Matrix\n",
    "\n",
    "# apply the relu (Rectified Linear Regression) function \n",
    "relu_hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "# define the parameters for the second layer\n",
    "W_layer2 = tf.Variable(tf.truncated_normal([hidden_layer_size, num_labels])) # [1024 x 10] Matrix\n",
    "\n",
    "# define the biases for the second layer \n",
    "b_layer2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "# training computation\n",
    "logits = tf.matmul(relu_hidden_layer, W_layer2) + b_layer2 # [128 x 10]\n",
    "\n",
    "# calculate cross entropy\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "# apply L2 regularization of the trained weights\n",
    "regularizers = tf.nn.l2_loss(W_layer1) + tf.nn.l2_loss(W_layer2)\n",
    "loss += 1e-4 * regularizers \n",
    "\n",
    "# optimize the loss function using gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "train_predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# run the validation dataset using the trained netword (weights and biases)\n",
    "valid_prediction_hidden_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, W_layer1) + b_layer1)\n",
    "valid_prediction = tf.nn.softmax(tf.matmul(valid_prediction_hidden_layer, W_layer2) + b_layer2)\n",
    "\n",
    "# run the test dataset in the trained netword (weights and biases)\n",
    "test_prediction_hidden_layer = tf.nn.relu(tf.matmul(tf_test_dataset, W_layer1) + b_layer1)\n",
    "test_prediction = tf.nn.softmax(tf.matmul(test_prediction_hidden_layer, W_layer2) + b_layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now train the network.\n",
    "- The same 2-layer Neural Network, on assignment 2, accomplished an accuracy of roughly 89.6%. Using L2 regularization, we get an improvement of nearly 0.3% on final Test accuracy: 89.9%, using 4e-4 L2 constant.\n",
    "\n",
    "- I noticed that using different values for the regularization multiple constant, we get different accuracy results on the test set. Bellow I registered the different final test accuracy values for different L2 multiple constants. \n",
    "\n",
    "- 5e-4 --> 89.3%\n",
    "- 4e-4 --> 89.9%\n",
    "- 3e-4 --> 89.0%\n",
    "- 2e-4 --> 89.5%\n",
    "- 1e-4 --> 89.7%\n",
    "\n",
    "Obviously, with the introduction of one more hyper-parameter, the combination of possible values for a optimal solution just increases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 399.356720\n",
      "Minibatch accuracy 6.2%\n",
      "Validation accuracy 15.7%\n",
      "Minibatch loss at step 500: 51.906380\n",
      "Minibatch accuracy 79.7%\n",
      "Validation accuracy 76.9%\n",
      "Minibatch loss at step 1000: 42.132526\n",
      "Minibatch accuracy 86.7%\n",
      "Validation accuracy 79.9%\n",
      "Minibatch loss at step 1500: 43.823505\n",
      "Minibatch accuracy 85.2%\n",
      "Validation accuracy 81.0%\n",
      "Minibatch loss at step 2000: 48.532589\n",
      "Minibatch accuracy 78.9%\n",
      "Validation accuracy 82.1%\n",
      "Minibatch loss at step 2500: 38.938141\n",
      "Minibatch accuracy 82.0%\n",
      "Validation accuracy 81.2%\n",
      "Minibatch loss at step 3000: 40.805653\n",
      "Minibatch accuracy 79.7%\n",
      "Validation accuracy 82.7%\n",
      "Test accuracy: 89.7%\n"
     ]
    }
   ],
   "source": [
    "# properly initialize the tensorflow variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# initialize the model in run the operation to initialize the variables\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    \n",
    "    opt, l, predictions = sess.run(\n",
    "      [optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy %.1f%%\" % accuracy(valid_prediction.eval(session=sess), valid_labels))\n",
    "        \n",
    "print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(session=sess), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4*128\n",
    "\n",
    "# define the input variable\n",
    "# the input variable will receive the image's pixels for every batch\n",
    "tf_train_dataset = tf.placeholder(tf.float32, [None, image_size * image_size]) # [128 x 784] matrix \n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "\n",
    "# load the valid and test datasets\n",
    "tf_valid_dataset = tf.constant(valid_dataset)\n",
    "tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "# define the hidden layer size\n",
    "hidden_layer_size = 1024\n",
    "\n",
    "# define the weights (parameters) of the first layer\n",
    "W_layer1 = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_layer_size])) # [784x1024] Matrix \n",
    "\n",
    "# define the biases for the firtst layer\n",
    "b_layer1 = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "\n",
    "# training computation\n",
    "hidden_layer = tf.matmul(tf_train_dataset, W_layer1) + b_layer1 # [128 x 1024] Matrix\n",
    "\n",
    "# apply the relu (Rectified Linear Regression) function \n",
    "relu_hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "# define the parameters for the second layer\n",
    "W_layer2 = tf.Variable(tf.truncated_normal([hidden_layer_size, num_labels])) # [1024 x 10] Matrix\n",
    "\n",
    "# define the biases for the second layer \n",
    "b_layer2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "# training computation\n",
    "logits = tf.matmul(relu_hidden_layer, W_layer2) + b_layer2 # [128 x 10]\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "# regularizers = tf.nn.l2_loss(W_layer1) + tf.nn.l2_loss(W_layer2)\n",
    "# loss += 5e-4 * regularizers \n",
    "\n",
    "# optimize the loss function using gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "train_predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# run the validation dataset using the trained netword (weights and biases)\n",
    "valid_prediction_hidden_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, W_layer1) + b_layer1)\n",
    "valid_prediction = tf.nn.softmax(tf.matmul(valid_prediction_hidden_layer, W_layer2) + b_layer2)\n",
    "\n",
    "# run the test dataset in the trained netword (weights and biases)\n",
    "test_prediction_hidden_layer = tf.nn.relu(tf.matmul(tf_test_dataset, W_layer1) + b_layer1)\n",
    "test_prediction = tf.nn.softmax(tf.matmul(test_prediction_hidden_layer, W_layer2) + b_layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 396.384552\n",
      "Minibatch accuracy 10.2%\n",
      "Validation accuracy 16.7%\n",
      "Minibatch loss at step 500: 0.000011\n",
      "Minibatch accuracy 100.0%\n",
      "Validation accuracy 58.1%\n",
      "Minibatch loss at step 1000: 0.000007\n",
      "Minibatch accuracy 100.0%\n",
      "Validation accuracy 58.1%\n",
      "Minibatch loss at step 1500: 0.000005\n",
      "Minibatch accuracy 100.0%\n",
      "Validation accuracy 58.1%\n",
      "Minibatch loss at step 2000: 0.000004\n",
      "Minibatch accuracy 100.0%\n",
      "Validation accuracy 58.1%\n",
      "Minibatch loss at step 2500: 0.000004\n",
      "Minibatch accuracy 100.0%\n",
      "Validation accuracy 58.0%\n",
      "Minibatch loss at step 3000: 0.000003\n",
      "Minibatch accuracy 100.0%\n",
      "Validation accuracy 58.0%\n",
      "Test accuracy: 64.8%\n"
     ]
    }
   ],
   "source": [
    "# properly initialize the tensorflow variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# initialize the model in run the operation to initialize the variables\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    # Lets try to overfit the model using a small portion of the global training set\n",
    "    # offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[0:(batch_size), :]\n",
    "    batch_labels = train_labels[0:(batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    \n",
    "    opt, l, predictions = sess.run(\n",
    "      [optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy %.1f%%\" % accuracy(valid_prediction.eval(session=sess), valid_labels))\n",
    "        \n",
    "print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(session=sess), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing only 101 (from 0 to 100) batches, each one of size 128, the final test accuracy was 82.3% (Without Regularization). If this approach was to overfit the model, I was expecting that the Minibaches accuracy went up. However, at step 100, the Minibatch accuracy was only 74.2%.\n",
    "\n",
    "A second approach is to shrink the traning data to be the size of a few batches and run a Gradient descent approach over that data many times. Restricting the training dataset to 128x4 = 512 training data points, and running it over 3001 steps (each of them applying the same 512 dataset), we can see the Minibatch accuracy go up to 100.0%. Which makes total sense, now we have an overfitted model in the training dataset. The test accuracy was 67.3%, which also makes sense since an overfitted model cannot generalize well.\n",
    "\n",
    "- Using drop out in the same example, with 0.5 probability, only over the training dataset. The Test Accuracy jumps to 85.4%. That proves the assertion that Dropout alleviates the model to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# define the input variable\n",
    "# the input variable will receive the image's pixels for every batch\n",
    "tf_train_dataset = tf.placeholder(tf.float32, [None, image_size * image_size]) # [128 x 784] matrix \n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "\n",
    "# load the valid and test datasets\n",
    "tf_valid_dataset = tf.constant(valid_dataset)\n",
    "tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "# define the hidden layer size\n",
    "hidden_layer_size = 1024\n",
    "\n",
    "# define the weights (parameters) of the first layer\n",
    "W_layer1 = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_layer_size])) # [784x1024] Matrix \n",
    "\n",
    "# define the biases for the firtst layer\n",
    "b_layer1 = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "\n",
    "# training computation\n",
    "hidden_layer = tf.matmul(tf_train_dataset, W_layer1) + b_layer1 # [128 x 1024] Matrix\n",
    "\n",
    "# apply the relu (Rectified Linear Regression) function \n",
    "relu_hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "# to alleviate overfitting, lets add dropout in between the layers\n",
    "# i.e. when the activations from the first layer are flowing to the\n",
    "# second.\n",
    "# lets create a variable to record the probability that an activation\n",
    "# will be dropped out or not.\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "relu_hidden_layer = tf.nn.dropout(relu_hidden_layer, keep_prob)\n",
    "\n",
    "# define the parameters for the second layer\n",
    "W_layer2 = tf.Variable(tf.truncated_normal([hidden_layer_size, num_labels])) # [1024 x 10] Matrix\n",
    "\n",
    "# define the biases for the second layer \n",
    "b_layer2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "# training computation\n",
    "logits = tf.matmul(relu_hidden_layer, W_layer2) + b_layer2 # [128 x 10]\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "regularizers = tf.nn.l2_loss(W_layer1) + tf.nn.l2_loss(W_layer2)\n",
    "loss += 5e-3 * regularizers \n",
    "\n",
    "# optimize the loss function using gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "train_predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# run the validation dataset using the trained netword (weights and biases)\n",
    "valid_prediction_hidden_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, W_layer1) + b_layer1)\n",
    "valid_prediction = tf.nn.softmax(tf.matmul(valid_prediction_hidden_layer, W_layer2) + b_layer2)\n",
    "\n",
    "# run the test dataset in the trained netword (weights and biases)\n",
    "test_prediction_hidden_layer = tf.nn.relu(tf.matmul(tf_test_dataset, W_layer1) + b_layer1)\n",
    "test_prediction = tf.nn.softmax(tf.matmul(test_prediction_hidden_layer, W_layer2) + b_layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using L2 regularization and Dropout on this deeper model, the final test accuracy jumps from 89.9% to 90.8%. Both using L2 constant as 4e-4 and Dropout 0.5 on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 2144.921875\n",
      "Minibatch accuracy 39.8%\n",
      "Validation accuracy 28.4%\n",
      "Minibatch loss at step 500: 128.687347\n",
      "Minibatch accuracy 91.4%\n",
      "Validation accuracy 81.3%\n",
      "Minibatch loss at step 1000: 11.063235\n",
      "Minibatch accuracy 86.7%\n",
      "Validation accuracy 84.6%\n",
      "Minibatch loss at step 1500: 1.486928\n",
      "Minibatch accuracy 91.4%\n",
      "Validation accuracy 84.5%\n",
      "Minibatch loss at step 2000: 0.841210\n",
      "Minibatch accuracy 85.9%\n",
      "Validation accuracy 84.1%\n",
      "Minibatch loss at step 2500: 0.669946\n",
      "Minibatch accuracy 88.3%\n",
      "Validation accuracy 84.8%\n",
      "Minibatch loss at step 3000: 0.793008\n",
      "Minibatch accuracy 88.3%\n",
      "Validation accuracy 84.5%\n",
      "Test accuracy: 90.8%\n"
     ]
    }
   ],
   "source": [
    "# properly initialize the tensorflow variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# initialize the model in run the operation to initialize the variables\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob: 0.5}\n",
    "    \n",
    "    opt, l, predictions = sess.run(\n",
    "      [optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy %.1f%%\" % accuracy(\n",
    "                train_predictions.eval(\n",
    "                    session=sess, feed_dict = {tf_train_dataset : batch_data, \n",
    "                                               tf_train_labels : batch_labels, \n",
    "                                               keep_prob: 1.0}), batch_labels))\n",
    "        print(\"Validation accuracy %.1f%%\" % \n",
    "              accuracy(valid_prediction.eval(session=sess), valid_labels))\n",
    "        \n",
    "print(\"Test accuracy: %.1f%%\" % \n",
    "      accuracy(test_prediction.eval(session=sess), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to increase the accuracy by adding more layers and by using learning rate decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# define the input variable\n",
    "# the input variable will receive the image's pixels for every batch\n",
    "tf_train_dataset = tf.placeholder(tf.float32, [None, image_size * image_size]) # [128 x 784] matrix \n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "\n",
    "# placeholder to determine the percentage of data that will be dropped out\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# load the valid and test datasets\n",
    "tf_valid_dataset = tf.constant(valid_dataset)\n",
    "tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "##############################################################\n",
    "## Layer 1\n",
    "##############################################################\n",
    "# define the hidden layer size\n",
    "hidden_layer_size = 1024\n",
    "\n",
    "# define the weights (parameters) of the first layer\n",
    "W_layer1 = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_layer_size], stddev=0.03)) # [784x1024] Matrix \n",
    "\n",
    "# define the biases for the firtst layer\n",
    "b_layer1 = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "\n",
    "# training computation\n",
    "hidden_layer = tf.matmul(tf_train_dataset, W_layer1) + b_layer1 # [128 x 1024] Matrix\n",
    "\n",
    "# apply the relu (Rectified Linear Regression) function \n",
    "relu_hidden_layer = tf.nn.dropout(tf.nn.relu(hidden_layer), keep_prob)\n",
    "\n",
    "##############################################################\n",
    "## Layer 2\n",
    "##############################################################\n",
    "# define the hidden layer size\n",
    "hidden_layer2_size = 1024\n",
    "\n",
    "# define the weights (parameters) of the first layer\n",
    "W_layer2 = tf.Variable(tf.truncated_normal([hidden_layer_size, hidden_layer2_size], stddev=0.03)) # [1024x2048] Matrix \n",
    "\n",
    "# define the biases for the firtst layer\n",
    "b_layer2 = tf.Variable(tf.zeros([hidden_layer2_size])) # [2048]\n",
    "\n",
    "# training computation\n",
    "hidden_layer2 = tf.matmul(relu_hidden_layer, W_layer2) + b_layer2 # [128 x 2048] Matrix\n",
    "\n",
    "# apply the relu (Rectified Linear Regression) function \n",
    "relu_hidden_layer2 = tf.nn.dropout(tf.nn.relu(hidden_layer2), keep_prob)\n",
    "\n",
    "##############################################################\n",
    "## Layer 3\n",
    "##############################################################\n",
    "# define the hidden layer size\n",
    "# hidden_layer3_size = 1024\n",
    "\n",
    "# define the weights (parameters) of the first layer\n",
    "# W_layer3 = tf.Variable(tf.truncated_normal([hidden_layer2_size, hidden_layer3_size], stddev=0.03)) # [2048x2048] Matrix \n",
    "\n",
    "# define the biases for the firtst layer\n",
    "# b_layer3 = tf.Variable(tf.zeros([hidden_layer3_size])) # [2048]\n",
    "\n",
    "# training computation\n",
    "# hidden_layer3 = tf.matmul(relu_hidden_layer2, W_layer3) + b_layer3 # [128 x 2048] Matrix\n",
    "\n",
    "# apply the relu (Rectified Linear Regression) function \n",
    "# relu_hidden_layer3 = tf.nn.dropout(tf.nn.relu(hidden_layer3), keep_prob)\n",
    "\n",
    "##############################################################\n",
    "## Layer 4\n",
    "##############################################################\n",
    "\n",
    "# define the parameters for the second layer\n",
    "W_layer3 = tf.Variable(tf.truncated_normal([hidden_layer2_size, num_labels], stddev=0.03)) # [1024 x 10] Matrix\n",
    "\n",
    "# define the biases for the second layer \n",
    "b_layer3 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "# training computation\n",
    "logits = tf.matmul(relu_hidden_layer2, W_layer3) + b_layer3 # [128 x 10]\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "# apply regularization\n",
    "regularizers = tf.nn.l2_loss(W_layer1) + tf.nn.l2_loss(W_layer2) + tf.nn.l2_loss(W_layer3)\n",
    "loss += 4e-4 * regularizers \n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.5\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           decay_steps = batch_size, \n",
    "                                           decay_rate = 0.95, \n",
    "                                           staircase=True)\n",
    "\n",
    "# optimize the loss function using gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "train_predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# run the validation dataset using the trained netword (weights and biases)\n",
    "valid_prediction_hidden_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, W_layer1) + b_layer1)\n",
    "valid_prediction_hidden_layer = tf.nn.relu(tf.matmul(valid_prediction_hidden_layer, W_layer2) + b_layer2)\n",
    "#valid_prediction_hidden_layer = tf.nn.relu(tf.matmul(valid_prediction_hidden_layer, W_layer3) + b_layer3)\n",
    "valid_prediction = tf.nn.softmax(tf.matmul(valid_prediction_hidden_layer, W_layer3) + b_layer3)\n",
    "\n",
    "# run the test dataset in the trained netword (weights and biases)\n",
    "test_prediction_hidden_layer = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_test_dataset, W_layer1) + b_layer1), 1.0)\n",
    "test_prediction_hidden_layer = tf.nn.dropout(tf.nn.relu(tf.matmul(test_prediction_hidden_layer, W_layer2) + b_layer2), 1.0)\n",
    "#test_prediction_hidden_layer = tf.nn.dropout(tf.nn.relu(tf.matmul(test_prediction_hidden_layer, W_layer3) + b_layer3), 1.0)\n",
    "test_prediction = tf.nn.softmax(tf.matmul(test_prediction_hidden_layer, W_layer3) + b_layer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to improve the final test accuracy, I made a few changes:\n",
    "    - Added a new layer with 2048 hidden neurons;\n",
    "    - Change the weights initialization from stddev=1.0 (Standard deviation - Default in tensorflow) to stddev=0.003 since small noise works better in deeper models.\n",
    "    - Added exponential learning rate decay (basically the learning rate decreases as the model learns)\n",
    "    \n",
    "We registered a final test accuracy of roughly 94%. Of course, better results are subjugated to better parameterization tunning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 2.600160\n",
      "Minibatch accuracy 8.6%\n",
      "Validation accuracy 21.8%\n",
      "Minibatch loss at step 500: 0.767629\n",
      "Minibatch accuracy 83.6%\n",
      "Validation accuracy 85.3%\n",
      "Minibatch loss at step 1000: 0.688453\n",
      "Minibatch accuracy 87.5%\n",
      "Validation accuracy 86.6%\n",
      "Minibatch loss at step 1500: 0.577422\n",
      "Minibatch accuracy 86.7%\n",
      "Validation accuracy 87.4%\n",
      "Minibatch loss at step 2000: 0.788710\n",
      "Minibatch accuracy 83.6%\n",
      "Validation accuracy 87.9%\n",
      "Minibatch loss at step 2500: 0.524377\n",
      "Minibatch accuracy 89.1%\n",
      "Validation accuracy 88.5%\n",
      "Minibatch loss at step 3000: 0.697827\n",
      "Minibatch accuracy 85.9%\n",
      "Validation accuracy 88.6%\n",
      "Test accuracy: 94.2%\n"
     ]
    }
   ],
   "source": [
    "# properly initialize the tensorflow variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# initialize the model in run the operation to initialize the variables\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "num_steps = 3001\n",
    "learning_rate_decay = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob: 0.5}\n",
    "    \n",
    "    opt, l, l_rate, predictions = sess.run(\n",
    "      [optimizer, loss, learning_rate, train_predictions], feed_dict=feed_dict)\n",
    "    \n",
    "    \n",
    "    learning_rate_decay.append(l_rate)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy %.1f%%\" % accuracy(valid_prediction.eval(session=sess), valid_labels))\n",
    "        \n",
    "print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(session=sess), test_labels))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# display a learning rate decreasing in a graph\n",
    "plt.plot(learning_rate_decay)\n",
    "plt.grid(1)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
